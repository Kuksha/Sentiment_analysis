{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T09:43:50.775129Z",
     "start_time": "2021-04-12T09:43:48.413218Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score ,StratifiedKFold,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T09:43:50.821758Z",
     "start_time": "2021-04-12T09:43:50.776130Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"products_sentiment_train.tsv\", sep = \"\\t\", header = None, names = [\"text\", \"label\"])\n",
    "test =  pd.read_csv(\"products_sentiment_test.tsv\", sep = \"\\t\", index_col = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T09:43:50.837762Z",
     "start_time": "2021-04-12T09:43:50.822759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 . take around 10,000 640x480 pictures .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the wrt54g plus the hga7t is a perfect solutio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont especially like how music files are uns...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was using the cheapie pail ... and it worked...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0          2 . take around 10,000 640x480 pictures .      1\n",
       "1  i downloaded a trial version of computer assoc...      1\n",
       "2  the wrt54g plus the hga7t is a perfect solutio...      1\n",
       "3  i dont especially like how music files are uns...      0\n",
       "4  i was using the cheapie pail ... and it worked...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "negfeats =  list(train[train.label == 0][\"text\"].values)\n",
    "posfeats =  list(train[train.label == 1][\"text\"].values)\n",
    "texts = train.text.values\n",
    "y = train.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1274\n",
      "726\n"
     ]
    }
   ],
   "source": [
    "print(len(posfeats))\n",
    "print(len(negfeats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовое решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T09:44:10.162745Z",
     "start_time": "2021-04-12T09:44:10.153744Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_pipeline(vectorizer, transformer, classifier):\n",
    "    return Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('transformer', transformer),\n",
    "            ('classifier', classifier)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T08:16:58.081708Z",
     "start_time": "2020-11-13T08:16:57.963988Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a1ded21a2141>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtexts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7655000000000001\n"
     ]
    }
   ],
   "source": [
    "pipe_base = make_pipeline(CountVectorizer(), TfidfTransformer(), LogisticRegression(solver = \"liblinear\"))\n",
    "result_base = cross_val_score(pipe_base, texts, y, cv =5, scoring=\"accuracy\")\n",
    "print(result_base.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор оптимального решения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейные классификаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression :  0.766\n",
      "LinearSVC :  0.7689999999999999\n",
      "SGDClassifier :  0.7545\n"
     ]
    }
   ],
   "source": [
    "for name, clf in [('LogisticRegression', LogisticRegression), \n",
    "                  ('LinearSVC', LinearSVC), \n",
    "                  ('SGDClassifier', SGDClassifier)]:\n",
    "    score = cross_val_score(make_pipeline(CountVectorizer(), TfidfTransformer(), clf(random_state=12)), texts, y, cv=5).mean()\n",
    "    print(name,\": \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#конструктор ценочной функции\n",
    "def make_estimator(classifier, params_grid, data, labels):\n",
    "    pipeline = make_pipeline(CountVectorizer(), TfidfTransformer(), classifier)\n",
    "    grid_cv = RandomizedSearchCV(pipeline, params_grid, scoring='accuracy', cv=5, random_state=12, n_iter=100,n_jobs=-1)\n",
    "    grid_cv.fit(data, labels)\n",
    "    return grid_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T08:29:34.570094Z",
     "start_time": "2020-11-13T08:29:34.557129Z"
    }
   },
   "outputs": [],
   "source": [
    "# Параметры для каждой модели\n",
    "\n",
    "params_grid_lr = {\n",
    "    'classifier__C': np.arange(0.1, 2, 0.1),\n",
    "    'classifier__max_iter': np.arange(50, 500, 50),\n",
    "    'classifier__solver': ['lbfgs', 'liblinear', 'sag']\n",
    "}\n",
    "params_grid_lsvc = {\n",
    "    'classifier__loss': ['hinge', 'squared_hinge'], \n",
    "    'classifier__max_iter': np.arange(100, 1000, 50),\n",
    "    'classifier__tol': [1e-5, 1e-4, 1e-3],\n",
    "    'classifier__C': np.arange(0.1, 2, 0.1)\n",
    "}\n",
    "params_grid_sgdc = {\n",
    "    'classifier__loss': ['log', 'hinge', 'modified_huber'], \n",
    "    'classifier__penalty':  ['l1', 'l2', 'elasticnet'], \n",
    "    'classifier__max_iter': np.arange(100, 1000, 50),\n",
    "    'classifier__tol': np.arange(1e-5, 1e-3, 1e-5),\n",
    "}\n",
    "\n",
    "# Параетры векторизации\n",
    "params_grid_vectorizer = {\n",
    "    'vectorizer__max_df' : [0.85, 0.9, 0.95, 1.0],\n",
    "    'vectorizer__min_df' : [1, 10, 20], \n",
    "    'vectorizer__ngram_range' : [(1, x) for x in range(1,7)],\n",
    "    'vectorizer__stop_words' : [ None, 'english']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression: 0.7585\n",
      "Wall time: 36.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_search_lr = make_estimator(LogisticRegression(), \n",
    "                                {**params_grid_vectorizer, **params_grid_lr}, texts, y)\n",
    "print(\"LogisticRegression: %.4f\" % grid_search_lr.best_score_ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC: 0.7905\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "grid_search_lsvc = make_estimator(LinearSVC(random_state=12), \n",
    "                                  {**params_grid_vectorizer, **params_grid_lsvc}, texts, y)\n",
    "print(\"LinearSVC: %.4f\" % grid_search_lsvc.best_score_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier: 0.7920\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "grid_search_sgdc = make_estimator(SGDClassifier(random_state=12), \n",
    "                                  {**params_grid_vectorizer, **params_grid_sgdc},  texts, y)\n",
    "print(\"SGDClassifier: %.4f\" % grid_search_sgdc.best_score_ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшая линейная модель: SGDClassifier, Accuracy = 0.7920\n",
      "Параметры модели: SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=200, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=12, shuffle=True, tol=0.00093,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "Парамтры предобработки: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=0.85, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 4), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Лучшая линейная модель: SGDClassifier, Accuracy = %.4f\"% grid_search_sgdc.best_score_)\n",
    "print(\"Параметры модели:\",  grid_search_sgdc.best_estimator_[2])\n",
    "print(\"Парамтры предобработки:\", grid_search_sgdc.best_estimator_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"color:red\"> UPD </p>\n",
    "Впоследствие окажется что это и есть лучшая модель, все более сложные модели, рассмотренные ниже, будут предсказывать с худшим качеством."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нелинейные алгоритмы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier :  0.7325000000000002\n",
      "RandomForestClassifier :  0.6415\n"
     ]
    }
   ],
   "source": [
    "for name, clf in [('GradientBoostingClassifier', GradientBoostingClassifier), \n",
    "                  ('RandomForestClassifier', RandomForestClassifier)]:\n",
    "    score = cross_val_score(make_pipeline(CountVectorizer(), TfidfTransformer(), clf(max_depth = 10,random_state=12)), texts, y, cv=5).mean()\n",
    "    print(name,\": \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_grid_gbt = {\n",
    "    \"classifier__learning_rate\": [0.01, 0.05,0.1, 0.15, 0.2],\n",
    "    \"classifier__min_samples_split\": np.linspace(0.05, 0.25, 5),\n",
    "    \"classifier__max_depth\":[3,5,8, 12],\n",
    "    \"classifier__max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"classifier__subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"classifier__n_estimators\": np.arange(100, 1000,100)\n",
    "}\n",
    "params_grid_rf = {\n",
    "    'classifier__n_estimators':np.arange(100, 1000,100),\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'classifier__max_depth': np.arange(2,16,2),\n",
    "    'classifier__criterion': ['gini', 'entropy'],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier: 0.7585\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_search_gbt = make_estimator(GradientBoostingClassifier(random_state=12), \n",
    "                                {**params_grid_vectorizer, **params_grid_gbt}, texts, y)\n",
    "print(\"GradientBoostingClassifier: %.4f\" % grid_search_gbt.best_score_ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier: 0.6640\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_search_rf = make_estimator(RandomForestClassifier(), \n",
    "                                {**params_grid_vectorizer, **params_grid_rf}, texts, y)\n",
    "print(\"RandomForestClassifier: %.4f\" % grid_search_rf.best_score_ )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выод: линейные алгоритмы показали себя лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем генерацию новых признаков с помощью метода главных компонент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x74561 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 114831 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfidfTransformer().fit_transform(CountVectorizer(ngram_range=(1,4)).fit_transform(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переопределим оценочную функцию\n",
    "def make_estimator(classifier, params_grid, data, labels):\n",
    "    pipeline =     Pipeline([\n",
    "            ('vectorizer', CountVectorizer()),\n",
    "            ('transformer', TfidfTransformer()),\n",
    "            (\"pca\", TruncatedSVD()),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "    grid_cv = RandomizedSearchCV(pipeline, params_grid, scoring='accuracy', cv=5, random_state=12, n_iter=100,n_jobs=-1, verbose =10)\n",
    "    grid_cv.fit(data, labels)\n",
    "    return grid_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_grid = {\n",
    "    \"pca__n_components\" : [10,100, 150, 200,250, 300, 350],\n",
    "    \"pca__n_iter\":[7, 10,15]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_grid_lsvc = {\n",
    "    'classifier__loss': ['hinge', 'squared_hinge'], \n",
    "    'classifier__max_iter': np.arange(100, 600, 100),\n",
    "    'classifier__tol': [1e-5, 1e-4, 1e-3],\n",
    "    'classifier__C': np.arange(0.1, 2, 0.1)\n",
    "}\n",
    "params_grid_sgdc = {\n",
    "    'classifier__loss': ['log', 'hinge', 'modified_huber'], \n",
    "    'classifier__penalty':  ['l1', 'l2', 'elasticnet'], \n",
    "    'classifier__max_iter': np.arange(100, 500, 100),\n",
    "    'classifier__tol': np.arange(1e-5, 1e-3, 1e-5),\n",
    "}\n",
    "\n",
    "\n",
    "params_grid_vectorizer = {\n",
    "    'vectorizer__max_df' : [0.85, 0.9, 0.95, 1.0],\n",
    "    'vectorizer__min_df' : [1, 10, 20], \n",
    "    'vectorizer__ngram_range' : [(1, x) for x in range(1,6)],\n",
    "    'vectorizer__stop_words' : [ None, 'english']\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_lsvc.best_estimator_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   31.8s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   38.1s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   44.5s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   45.4s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   46.7s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC: 0.7665\n",
      "TruncatedSVD(algorithm='randomized', n_components=200, n_iter=7,\n",
      "             random_state=None, tol=0.0)\n"
     ]
    }
   ],
   "source": [
    "grid_search_lsvc = make_estimator(LinearSVC(random_state=12), \n",
    "                                  {**params_grid_vectorizer, **params_grid_lsvc, **pca_grid}, texts, y)\n",
    "print(\"LinearSVC: %.4f\" % grid_search_lsvc.best_score_ )\n",
    "print(grid_search_lsvc.best_estimator_[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1106s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too slow (6.8692s.) Setting batch_size=1.\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:   20.4s\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:   31.9s\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 127 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 161 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 180 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 199 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 220 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 287 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 312 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 337 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 364 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 391 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 420 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression: 0.7610\n",
      "TruncatedSVD: TruncatedSVD(algorithm='randomized', n_components=150, n_iter=10,\n",
      "             random_state=None, tol=0.0)\n"
     ]
    }
   ],
   "source": [
    "grid_search_lr = make_estimator(LogisticRegression(), \n",
    "                                  {**params_grid_vectorizer, **params_grid_lr, **pca_grid}, texts, y)\n",
    "print(\"LinearRegression: %.4f\" % grid_search_lr.best_score_ )\n",
    "print(\"TruncatedSVD:\", grid_search_lr.best_estimator_[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1812s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:   26.6s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too slow (6.7118s.) Setting batch_size=1.\n",
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:   45.4s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  99 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 163 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 182 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 222 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 243 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 266 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 314 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 339 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 366 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 393 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 422 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 451 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 482 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  9.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier: 0.7730\n",
      "Wall time: 9min 46s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "grid_search_sgdc = make_estimator(SGDClassifier(random_state=12), \n",
    "                                  {**params_grid_vectorizer, **params_grid_sgdc,**pca_grid},  texts, y)\n",
    "print(\"SGDClassifier: %.4f\" % grid_search_sgdc.best_score_ )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод: Отображение пространство преобразованных признаков с помощью метода главных компонент не приводит к улучшению результата, а ухудшает его.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итоговая модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучше всего себя показала модель стохастическго градиентного спуска. Уточним параметры этой модели для получения наилучшего качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определим функцию заново, так как вышеприведённый эксперимент не удался\n",
    "def make_estimator(classifier, params_grid, data, labels):\n",
    "    pipeline = make_pipeline(CountVectorizer(), TfidfTransformer(), classifier)\n",
    "    grid_cv = RandomizedSearchCV(pipeline, params_grid, scoring='accuracy', cv=5, random_state=12, n_iter=1000,n_jobs=-1,)\n",
    "    grid_cv.fit(data, labels)\n",
    "    return grid_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=0.985,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 5), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabu...\n",
       "                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.05, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.5,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=300, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=None,\n",
       "                               shuffle=True, tol=0.000855,\n",
       "                               validation_fraction=0.1, verbose=0,\n",
       "                               warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_sgdc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid_sgdc = {\n",
    "    'classifier__loss': ['log', 'hinge', 'modified_huber'], \n",
    "    'classifier__penalty':  ['l1', 'l2', 'elasticnet'], \n",
    "    'classifier__max_iter': np.arange(100, 1000, 25),\n",
    "    'classifier__tol': np.arange(1e-5, 1e-3, 5*1e-6),\n",
    "    'classifier__l1_ratio': [0, 0.05, 0.1, 0.2, 0.5],\n",
    "    \"classifier__epsilon\":[0.5,0.1,0.15,0.2]\n",
    "}\n",
    "\n",
    "params_grid_vectorizer = {\n",
    "    'vectorizer__max_df' : np.arange(0.94,0.99,0.005),\n",
    "    'vectorizer__min_df' : [1, 2], \n",
    "    'vectorizer__ngram_range' : [(1, x) for x in range(1,6)],\n",
    "    'vectorizer__stop_words' : [ None, 'english']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier: 0.7935\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_search_sgdc = make_estimator(SGDClassifier(random_state=None), \n",
    "                                  {**params_grid_vectorizer, **params_grid_sgdc},  texts, y)\n",
    "print(\"SGDClassifier: %.4f\" % grid_search_sgdc.best_score_ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier: 0.7935\n"
     ]
    }
   ],
   "source": [
    "print(\"SGDClassifier: %.4f\" % grid_search_sgdc.best_score_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=0.965,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 3), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabu...\n",
       "                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.2, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.5,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=925, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=None,\n",
       "                               shuffle=True, tol=0.0005949999999999999,\n",
       "                               validation_fraction=0.1, verbose=0,\n",
       "                               warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_sgdc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = grid_search_sgdc.best_estimator_.predict(test.text.values)\n",
    "df = pd.Series(arr).to_frame()\n",
    "df.columns = [ \"y\"]\n",
    "df.index.name = \"Id\"\n",
    "df.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результат\n",
    "![title](result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
